2025-07-01T14:26:05 (diar.sh:141:main) ./diar.sh --collar 0.0 --train_set train --valid_set dev --test_sets test --ngpu 1 --diar_config conf/train_diar_eda.yaml --inference_config conf/decode_diar_eda.yaml --inference_nj 5 --local_data_opts --num_spk 2 --stop_stage 5
2025-07-01T14:26:07 (diar.sh:193:main) Stage 1: Data preparation for data/train, data/dev, etc.
2025-07-01T14:26:08 (data.sh:34:main) data preparation started
[Warning] - train-clean-360 is ignored in create_librimix_from_metadata.py for less data preparation time. Please note that in S3PRL we only use the train-clean-100 for downstream tasks.
Directory downloads/Libri2Mix/wav8k/max/train-100 already exist. Files won't be overwritten
Directory downloads/Libri2Mix/wav8k/max/test already exist. Files won't be overwritten
Directory downloads/Libri2Mix/wav8k/max/dev already exist. Files won't be overwritten
[Warning] - train-clean-360 is ignored in create_librimix_from_metadata.py for less data preparation time. Please note that in S3PRL we only use the train-clean-100 for downstream tasks.
Directory downloads/Libri3Mix/wav8k/max/dev already exist. Files won't be overwritten
Directory downloads/Libri3Mix/wav8k/max/test already exist. Files won't be overwritten
Directory downloads/Libri3Mix/wav8k/max/train-100 already exist. Files won't be overwritten
Successfully finish Kaldi-style preparation
utils/fix_data_dir.sh: file data/test/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/spk2utt is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/segments is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/wav.scp is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/reco2dur is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 7119 utterances.
fix_data_dir.sh: old files are kept in data/test/.backup
utils/fix_data_dir.sh: file data/train/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/spk2utt is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/segments is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/wav.scp is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/reco2dur is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 61890 utterances.
fix_data_dir.sh: old files are kept in data/train/.backup
utils/fix_data_dir.sh: file data/dev/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/spk2utt is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/segments is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/wav.scp is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/reco2dur is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 7251 utterances.
fix_data_dir.sh: old files are kept in data/dev/.backup
2025-07-01T14:26:54 (data.sh:91:main) Successfully finished. [elapsed=47s]
2025-07-01T14:26:54 (diar.sh:200:main) Stage 2: Format wav.scp: data/ -> dump/raw
utils/copy_data_dir.sh: copied data from data/train to dump/raw/org/train
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/org/train
2025-07-01T14:26:55 (format_wav_scp.sh:46:main) scripts/audio/format_wav_scp.sh --nj 32 --cmd slurm.pl --num-threads 4 --mem 1900M --audio-format flac --fs 8k data/train/wav.scp dump/raw/org/train
2025-07-01T14:26:57 (format_wav_scp.sh:118:main) [info]: without segments
2025-07-01T14:28:55 (format_wav_scp.sh:153:main) Successfully finished. [elapsed=120s]
2025-07-01 14:35:05,625 (convert_rttm:136) INFO: Successfully finished RTTM converting.
utils/copy_data_dir.sh: copied data from data/dev to dump/raw/org/dev
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/org/dev
2025-07-01T14:35:06 (format_wav_scp.sh:46:main) scripts/audio/format_wav_scp.sh --nj 32 --cmd slurm.pl --num-threads 4 --mem 1900M --audio-format flac --fs 8k data/dev/wav.scp dump/raw/org/dev
2025-07-01T14:35:07 (format_wav_scp.sh:118:main) [info]: without segments
2025-07-01T14:36:55 (format_wav_scp.sh:153:main) Successfully finished. [elapsed=109s]
2025-07-01 14:37:43,032 (convert_rttm:136) INFO: Successfully finished RTTM converting.
utils/copy_data_dir.sh: copied data from data/test to dump/raw/test
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/test
2025-07-01T14:37:43 (format_wav_scp.sh:46:main) scripts/audio/format_wav_scp.sh --nj 32 --cmd slurm.pl --num-threads 4 --mem 1900M --audio-format flac --fs 8k data/test/wav.scp dump/raw/test
2025-07-01T14:37:44 (format_wav_scp.sh:118:main) [info]: without segments
2025-07-01T14:38:38 (format_wav_scp.sh:153:main) Successfully finished. [elapsed=55s]
2025-07-01 14:39:26,815 (convert_rttm:136) INFO: Successfully finished RTTM converting.
2025-07-01T14:39:26 (diar.sh:241:main) Stage 3: Remove short data: dump/raw/org -> dump/raw
utils/copy_data_dir.sh: copied data from dump/raw/org/train to dump/raw/train
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/train
fix_data_dir.sh: kept all 61890 utterances.
fix_data_dir.sh: old files are kept in dump/raw/train/.backup
2025-07-01 14:39:38,970 (convert_rttm:136) INFO: Successfully finished RTTM converting.
utils/copy_data_dir.sh: copied data from dump/raw/org/dev to dump/raw/dev
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/dev
fix_data_dir.sh: kept all 7251 utterances.
fix_data_dir.sh: old files are kept in dump/raw/dev/.backup
2025-07-01 14:39:42,503 (convert_rttm:136) INFO: Successfully finished RTTM converting.
2025-07-01T14:39:42 (diar.sh:293:main) Stage 4: Diarization collect stats: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-07-01T14:39:42 (diar.sh:343:main) Generate 'exp/diar_stats_8k/run.sh'. You can resume the process from stage 4 using this script
2025-07-01T14:39:42 (diar.sh:347:main) Diarization collect-stats started... log: 'exp/diar_stats_8k/logdir/stats.*.log'
/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1/utils/slurm.pl: 32 / 32 failed, log is in exp/diar_stats_8k/logdir/stats.*.log
# Running on r014.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r014
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=1
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283261
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283261
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r014
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r014
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=81418
# SLURM_TOPOLOGY_ADDR=r014
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.1.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.1.scp --output_dir exp/diar_stats_8k/logdir/stats.1 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.1.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.1.scp --output_dir exp/diar_stats_8k/logdir/stats.1 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r014] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r014] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r014] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r014] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r014] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r014] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r014] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r014] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.1/config.yaml
[r014] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.1', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.1.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.1.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r014] 2025-07-01 14:43:07,703 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r263.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r263
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=10
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283270
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283270
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r263
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r263
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=45285
# SLURM_TOPOLOGY_ADDR=r263
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.10.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.10.scp --output_dir exp/diar_stats_8k/logdir/stats.10 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.10.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.10.scp --output_dir exp/diar_stats_8k/logdir/stats.10 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r263] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r263] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r263] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r263] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r263] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r263] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r263] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r263] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r263] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r263] 2025-07-01 14:43:06,820 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r263] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r263] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r263] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r263] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.10/config.yaml
[r263] 2025-07-01 14:43:06,860 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.10', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.10.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.10.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r263] 2025-07-01 14:43:07,523 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r267.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r267
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=11
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283271
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283271
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r267
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r267
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=19533
# SLURM_TOPOLOGY_ADDR=r267
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.11.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.11.scp --output_dir exp/diar_stats_8k/logdir/stats.11 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.11.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.11.scp --output_dir exp/diar_stats_8k/logdir/stats.11 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r267] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r267] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r267] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r267] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r267] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r267] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r267] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.11/config.yaml
[r267] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.11', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.11.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.11.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r267] 2025-07-01 14:43:07,577 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r278.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r278
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=12
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283272
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283272
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r278
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r278
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=104795
# SLURM_TOPOLOGY_ADDR=r278
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.12.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.12.scp --output_dir exp/diar_stats_8k/logdir/stats.12 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.12.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.12.scp --output_dir exp/diar_stats_8k/logdir/stats.12 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r278] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r278] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r278] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r278] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r278] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r278] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r278] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r278] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r278] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r278] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.12/config.yaml
[r278] 2025-07-01 14:43:06,860 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.12', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.12.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.12.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r278] 2025-07-01 14:43:07,488 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r283.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r283
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=13
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283273
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283273
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r283
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r283
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=41317
# SLURM_TOPOLOGY_ADDR=r283
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.13.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.13.scp --output_dir exp/diar_stats_8k/logdir/stats.13 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.13.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.13.scp --output_dir exp/diar_stats_8k/logdir/stats.13 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r283] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r283] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r283] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r283] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r283] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r283] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r283] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r283] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.13/config.yaml
[r283] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.13', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.13.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.13.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r283] 2025-07-01 14:43:07,609 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r291.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r291
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=14
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283274
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283274
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r291
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r291
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=90409
# SLURM_TOPOLOGY_ADDR=r291
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.14.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.14.scp --output_dir exp/diar_stats_8k/logdir/stats.14 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.14.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.14.scp --output_dir exp/diar_stats_8k/logdir/stats.14 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r291] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r291] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r291] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r291] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r291] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r291] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r291] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r291] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r291] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r291] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r291] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r291] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r291] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.14/config.yaml
[r291] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.14', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.14.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.14.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r291] 2025-07-01 14:43:07,602 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r314.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r314
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=15
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283275
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283275
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r314
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r314
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=17886
# SLURM_TOPOLOGY_ADDR=r314
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.15.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.15.scp --output_dir exp/diar_stats_8k/logdir/stats.15 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.15.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.15.scp --output_dir exp/diar_stats_8k/logdir/stats.15 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r314] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r314] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r314] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r314] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r314] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r314] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r314] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r314] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r314] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r314] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r314] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r314] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r314] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.15/config.yaml
[r314] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.15', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.15.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.15.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r314] 2025-07-01 14:43:07,660 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r319.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r319
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=16
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283276
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283276
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r319
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r319
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=81194
# SLURM_TOPOLOGY_ADDR=r319
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.16.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.16.scp --output_dir exp/diar_stats_8k/logdir/stats.16 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.16.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.16.scp --output_dir exp/diar_stats_8k/logdir/stats.16 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r319] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r319] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r319] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r319] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r319] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r319] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r319] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r319] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r319] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r319] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r319] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r319] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r319] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r319] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.16/config.yaml
[r319] 2025-07-01 14:43:06,860 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.16', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.16.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.16.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r319] 2025-07-01 14:43:07,650 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r366.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r366
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=17
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283277
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283277
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r366
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r366
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=13171
# SLURM_TOPOLOGY_ADDR=r366
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.17.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.17.scp --output_dir exp/diar_stats_8k/logdir/stats.17 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.17.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.17.scp --output_dir exp/diar_stats_8k/logdir/stats.17 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r366] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r366] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r366] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r366] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r366] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r366] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r366] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r366] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.17/config.yaml
[r366] 2025-07-01 14:43:06,862 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.17', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.17.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.17.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r366] 2025-07-01 14:43:07,614 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r374.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r374
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=18
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283278
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283278
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r374
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r374
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=79180
# SLURM_TOPOLOGY_ADDR=r374
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.18.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.18.scp --output_dir exp/diar_stats_8k/logdir/stats.18 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.18.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.18.scp --output_dir exp/diar_stats_8k/logdir/stats.18 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r374] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r374] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r374] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r374] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r374] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r374] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r374] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r374] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r374] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r374] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r374] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r374] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.18/config.yaml
[r374] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.18', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.18.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.18.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r374] 2025-07-01 14:43:07,647 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r376.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r376
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=19
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283279
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283279
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r376
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r376
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=98661
# SLURM_TOPOLOGY_ADDR=r376
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.19.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.19.scp --output_dir exp/diar_stats_8k/logdir/stats.19 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.19.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.19.scp --output_dir exp/diar_stats_8k/logdir/stats.19 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r376] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r376] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r376] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r376] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r376] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r376] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r376] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r376] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r376] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.19/config.yaml
[r376] 2025-07-01 14:43:06,859 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.19', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.19.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.19.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r376] 2025-07-01 14:43:07,409 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r017.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r017
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=2
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283262
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283262
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r017
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r017
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=65948
# SLURM_TOPOLOGY_ADDR=r017
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.2.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.2.scp --output_dir exp/diar_stats_8k/logdir/stats.2 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.2.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.2.scp --output_dir exp/diar_stats_8k/logdir/stats.2 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r017] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r017] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r017] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r017] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r017] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r017] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r017] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.2/config.yaml
[r017] 2025-07-01 14:43:06,859 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.2', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.2.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.2.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r017] 2025-07-01 14:43:07,614 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r381.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r381
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=20
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283280
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283280
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r381
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r381
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=15612
# SLURM_TOPOLOGY_ADDR=r381
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.20.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.20.scp --output_dir exp/diar_stats_8k/logdir/stats.20 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.20.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.20.scp --output_dir exp/diar_stats_8k/logdir/stats.20 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r381] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r381] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r381] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r381] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r381] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r381] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r381] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r381] 2025-07-01 14:43:06,858 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.20/config.yaml
[r381] 2025-07-01 14:43:06,867 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.20', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.20.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.20.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r381] 2025-07-01 14:43:07,682 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r395.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r395
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=21
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283281
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283281
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r395
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r395
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=19068
# SLURM_TOPOLOGY_ADDR=r395
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.21.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.21.scp --output_dir exp/diar_stats_8k/logdir/stats.21 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.21.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.21.scp --output_dir exp/diar_stats_8k/logdir/stats.21 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r395] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r395] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r395] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r395] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r395] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r395] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r395] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r395] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r395] 2025-07-01 14:43:06,861 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.21/config.yaml
[r395] 2025-07-01 14:43:06,870 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.21', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.21.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.21.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r395] 2025-07-01 14:43:07,772 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r019.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r019
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=22
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283282
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283282
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r019
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r019
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=73202
# SLURM_TOPOLOGY_ADDR=r019
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.22.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.22.scp --output_dir exp/diar_stats_8k/logdir/stats.22 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.22.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.22.scp --output_dir exp/diar_stats_8k/logdir/stats.22 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r019] 2025-07-01 14:43:06,820 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r019] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r019] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r019] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r019] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.22/config.yaml
[r019] 2025-07-01 14:43:06,867 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.22', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.22.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.22.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r019] 2025-07-01 14:43:07,985 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r019.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r019
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=23
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283283
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283283
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r019
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r019
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=73203
# SLURM_TOPOLOGY_ADDR=r019
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.23.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.23.scp --output_dir exp/diar_stats_8k/logdir/stats.23 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.23.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.23.scp --output_dir exp/diar_stats_8k/logdir/stats.23 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r019] 2025-07-01 14:42:40,458 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,459 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r019] 2025-07-01 14:42:40,460 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r019] 2025-07-01 14:42:40,461 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r019] 2025-07-01 14:43:06,820 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r019] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r019] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r019] 2025-07-01 14:43:06,851 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r019] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.23/config.yaml
[r019] 2025-07-01 14:43:06,868 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.23', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.23.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.23.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r019] 2025-07-01 14:43:07,985 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r028.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r028
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=24
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283284
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283284
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r028
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r028
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=41433
# SLURM_TOPOLOGY_ADDR=r028
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.24.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.24.scp --output_dir exp/diar_stats_8k/logdir/stats.24 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.24.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.24.scp --output_dir exp/diar_stats_8k/logdir/stats.24 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r028] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r028] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r028] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r028] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r028] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.24/config.yaml
[r028] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.24', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.24.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.24.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r028] 2025-07-01 14:43:07,643 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r028.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r028
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=25
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283285
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283285
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r028
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r028
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=41432
# SLURM_TOPOLOGY_ADDR=r028
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.25.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.25.scp --output_dir exp/diar_stats_8k/logdir/stats.25 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.25.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.25.scp --output_dir exp/diar_stats_8k/logdir/stats.25 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r028] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r028] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r028] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r028] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r028] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r028] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r028] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.25/config.yaml
[r028] 2025-07-01 14:43:06,860 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.25', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.25.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.25.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r028] 2025-07-01 14:43:07,642 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r046.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:28 EDT 2025
# SLURMD_NODENAME=r046
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=26
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283286
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283286
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r046
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r046
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=81398
# SLURM_TOPOLOGY_ADDR=r046
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.26.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.26.scp --output_dir exp/diar_stats_8k/logdir/stats.26 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.26.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.26.scp --output_dir exp/diar_stats_8k/logdir/stats.26 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r046] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r046] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r046] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r046] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r046] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.26/config.yaml
[r046] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.26', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.26.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.26.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r046] 2025-07-01 14:43:07,699 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395228
# Accounting: end_time=1751395397
# Accounting: time=169 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r046.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:28 EDT 2025
# SLURMD_NODENAME=r046
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=27
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283287
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283287
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r046
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r046
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=81401
# SLURM_TOPOLOGY_ADDR=r046
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.27.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.27.scp --output_dir exp/diar_stats_8k/logdir/stats.27 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.27.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.27.scp --output_dir exp/diar_stats_8k/logdir/stats.27 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r046] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r046] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r046] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r046] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r046] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r046] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r046] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.27/config.yaml
[r046] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.27', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.27.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.27.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r046] 2025-07-01 14:43:07,701 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395228
# Accounting: end_time=1751395397
# Accounting: time=169 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r176.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:28 EDT 2025
# SLURMD_NODENAME=r176
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=28
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283288
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283288
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r176
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r176
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=67703
# SLURM_TOPOLOGY_ADDR=r176
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.28.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.28.scp --output_dir exp/diar_stats_8k/logdir/stats.28 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.28.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.28.scp --output_dir exp/diar_stats_8k/logdir/stats.28 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r176] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r176] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r176] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r176] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r176] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r176] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r176] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r176] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r176] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r176] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.28/config.yaml
[r176] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.28', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.28.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.28.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r176] 2025-07-01 14:43:07,666 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395228
# Accounting: end_time=1751395397
# Accounting: time=169 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r176.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:28 EDT 2025
# SLURMD_NODENAME=r176
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=29
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283289
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283289
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r176
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r176
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=67704
# SLURM_TOPOLOGY_ADDR=r176
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.29.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.29.scp --output_dir exp/diar_stats_8k/logdir/stats.29 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.29.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.29.scp --output_dir exp/diar_stats_8k/logdir/stats.29 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r176] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r176] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r176] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r176] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r176] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r176] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r176] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r176] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.29/config.yaml
[r176] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.29', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.29.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.29.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r176] 2025-07-01 14:43:07,665 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395228
# Accounting: end_time=1751395397
# Accounting: time=169 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r026.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r026
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=3
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283263
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283263
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r026
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r026
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=82433
# SLURM_TOPOLOGY_ADDR=r026
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.3.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.3.scp --output_dir exp/diar_stats_8k/logdir/stats.3 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.3.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.3.scp --output_dir exp/diar_stats_8k/logdir/stats.3 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r026] 2025-07-01 14:42:40,445 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r026] 2025-07-01 14:42:40,446 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r026] 2025-07-01 14:42:40,447 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r026] 2025-07-01 14:42:40,448 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r026] 2025-07-01 14:42:40,448 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r026] 2025-07-01 14:42:40,448 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r026] 2025-07-01 14:42:40,448 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r026] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r026] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r026] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r026] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r026] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.3/config.yaml
[r026] 2025-07-01 14:43:06,862 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.3', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.3.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.3.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r026] 2025-07-01 14:43:07,620 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395399
# Accounting: time=185 threads=1
# Finished at Tue Jul 1 14:43:19 EDT 2025 with status 1
# Running on r181.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r181
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=30
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283290
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283290
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r181
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r181
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=46492
# SLURM_TOPOLOGY_ADDR=r181
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.30.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.30.scp --output_dir exp/diar_stats_8k/logdir/stats.30 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.30.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.30.scp --output_dir exp/diar_stats_8k/logdir/stats.30 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r181] 2025-07-01 14:43:06,820 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r181] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r181] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r181] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r181] 2025-07-01 14:43:06,852 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.30/config.yaml
[r181] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.30', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.30.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.30.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r181] 2025-07-01 14:43:07,606 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r181.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r181
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=31
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283291
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283291
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r181
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r181
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=46491
# SLURM_TOPOLOGY_ADDR=r181
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.31.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.31.scp --output_dir exp/diar_stats_8k/logdir/stats.31 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.31.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.31.scp --output_dir exp/diar_stats_8k/logdir/stats.31 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r181] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r181] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r181] 2025-07-01 14:43:06,820 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r181] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r181] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r181] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r181] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.31/config.yaml
[r181] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.31', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.31.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.31.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r181] 2025-07-01 14:43:07,606 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r192.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r192
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=32
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283133
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283133
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r192
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r192
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=38921
# SLURM_TOPOLOGY_ADDR=r192
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.32.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.32.scp --output_dir exp/diar_stats_8k/logdir/stats.32 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.32.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.32.scp --output_dir exp/diar_stats_8k/logdir/stats.32 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r192] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r192] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r192] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r192] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r192] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r192] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r192] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r192] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.32/config.yaml
[r192] 2025-07-01 14:43:06,860 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.32', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.32.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.32.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r192] 2025-07-01 14:43:07,484 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r042.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r042
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=4
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283264
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283264
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r042
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r042
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=79272
# SLURM_TOPOLOGY_ADDR=r042
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.4.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.4.scp --output_dir exp/diar_stats_8k/logdir/stats.4 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.4.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.4.scp --output_dir exp/diar_stats_8k/logdir/stats.4 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r042] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r042] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r042] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r042] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r042] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r042] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r042] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r042] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.4/config.yaml
[r042] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.4', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.4.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.4.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r042] 2025-07-01 14:43:07,609 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r194.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r194
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=5
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283265
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283265
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r194
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r194
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=23668
# SLURM_TOPOLOGY_ADDR=r194
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.5.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.5.scp --output_dir exp/diar_stats_8k/logdir/stats.5 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.5.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.5.scp --output_dir exp/diar_stats_8k/logdir/stats.5 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r194] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r194] 2025-07-01 14:42:40,437 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r194] 2025-07-01 14:42:40,438 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r194] 2025-07-01 14:42:40,439 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r194] 2025-07-01 14:43:06,820 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r194] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r194] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r194] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r194] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.5/config.yaml
[r194] 2025-07-01 14:43:06,862 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.5', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.5.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.5.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r194] 2025-07-01 14:43:07,854 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r205.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:14 EDT 2025
# SLURMD_NODENAME=r205
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=6
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283266
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283266
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r205
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r205
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=20093
# SLURM_TOPOLOGY_ADDR=r205
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.6.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.6.scp --output_dir exp/diar_stats_8k/logdir/stats.6 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.6.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.6.scp --output_dir exp/diar_stats_8k/logdir/stats.6 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r205] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r205] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r205] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r205] 2025-07-01 14:42:40,436 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r205] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r205] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r205] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r205] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r205] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.6/config.yaml
[r205] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.6', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.6.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.6.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r205] 2025-07-01 14:43:07,642 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395214
# Accounting: end_time=1751395397
# Accounting: time=183 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r232.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r232
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=7
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283267
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283267
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r232
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r232
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=18458
# SLURM_TOPOLOGY_ADDR=r232
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.7.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.7.scp --output_dir exp/diar_stats_8k/logdir/stats.7 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.7.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.7.scp --output_dir exp/diar_stats_8k/logdir/stats.7 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r232] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r232] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r232] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r232] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r232] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r232] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r232] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r232] 2025-07-01 14:42:40,434 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r232] 2025-07-01 14:42:40,435 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r232] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r232] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r232] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r232] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r232] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.7/config.yaml
[r232] 2025-07-01 14:43:06,861 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.7', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.7.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.7.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r232] 2025-07-01 14:43:07,744 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r245.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r245
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=8
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283268
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283268
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r245
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r245
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=24419
# SLURM_TOPOLOGY_ADDR=r245
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.8.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.8.scp --output_dir exp/diar_stats_8k/logdir/stats.8 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.8.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.8.scp --output_dir exp/diar_stats_8k/logdir/stats.8 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r245] 2025-07-01 14:42:40,431 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r245] 2025-07-01 14:42:40,432 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r245] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r245] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r245] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r245] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r245] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r245] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r245] 2025-07-01 14:42:40,433 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r245] 2025-07-01 14:43:06,820 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r245] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r245] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r245] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r245] 2025-07-01 14:43:06,851 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.8/config.yaml
[r245] 2025-07-01 14:43:06,862 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.8', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.8.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.8.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r245] 2025-07-01 14:43:07,759 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
# Running on r256.ib.bridges2.psc.edu
# Started at Tue Jul 1 14:40:13 EDT 2025
# SLURMD_NODENAME=r256
# SLURM_ARRAY_JOB_ID=33283133
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=9
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=4
# SLURM_CPUS_PER_TASK=4
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=33283269
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=4
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=33283269
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r256
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r256
# SLURM_NODE_ALIASES='(null)'
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=br013.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=12424
# SLURM_TOPOLOGY_ADDR=r256
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.9.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.9.scp --output_dir exp/diar_stats_8k/logdir/stats.9 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_filename
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.9.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.9.scp --output_dir exp/diar_stats_8k/logdir/stats.9 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(
[r256] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r256] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r256] 2025-07-01 14:42:40,428 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r256] 2025-07-01 14:42:40,429 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r256] 2025-07-01 14:42:40,430 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r256] 2025-07-01 14:43:06,819 (abs_task:1398) INFO: pytorch.version=2.7.1+cu126, cuda.available=False, cudnn.version=90501, cudnn.benchmark=False, cudnn.deterministic=True
[r256] 2025-07-01 14:43:06,850 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r256] 2025-07-01 14:43:06,850 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r256] 2025-07-01 14:43:06,850 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r256] 2025-07-01 14:43:06,868 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.9/config.yaml
[r256] 2025-07-01 14:43:06,876 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.9', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.9.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.9.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r256] 2025-07-01 14:43:07,485 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1751395213
# Accounting: end_time=1751395397
# Accounting: time=184 threads=1
# Finished at Tue Jul 1 14:43:17 EDT 2025 with status 1
