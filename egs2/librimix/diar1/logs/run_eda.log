2025-06-18T22:17:17 (diar.sh:141:main) ./diar.sh --collar 0.0 --train_set train --valid_set dev --test_sets test --ngpu 1 --diar_config conf/train_diar_eda.yaml --inference_config conf/decode_diar_eda.yaml --inference_nj 5 --local_data_opts --num_spk 2 --stop_stage 5
2025-06-18T22:17:18 (diar.sh:193:main) Stage 1: Data preparation for data/train, data/dev, etc.
2025-06-18T22:17:19 (data.sh:34:main) data preparation started
[Warning] - train-clean-360 is ignored in create_librimix_from_metadata.py for less data preparation time. Please note that in S3PRL we only use the train-clean-100 for downstream tasks.
Directory downloads/Libri2Mix/wav8k/max/train-100 already exist. Files won't be overwritten
Directory downloads/Libri2Mix/wav8k/max/test already exist. Files won't be overwritten
Directory downloads/Libri2Mix/wav8k/max/dev already exist. Files won't be overwritten
[Warning] - train-clean-360 is ignored in create_librimix_from_metadata.py for less data preparation time. Please note that in S3PRL we only use the train-clean-100 for downstream tasks.
Directory downloads/Libri3Mix/wav8k/max/dev already exist. Files won't be overwritten
Directory downloads/Libri3Mix/wav8k/max/test already exist. Files won't be overwritten
Directory downloads/Libri3Mix/wav8k/max/train-100 already exist. Files won't be overwritten
Successfully finish Kaldi-style preparation
utils/fix_data_dir.sh: file data/test/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/spk2utt is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/segments is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/wav.scp is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/reco2dur is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 7119 utterances.
fix_data_dir.sh: old files are kept in data/test/.backup
utils/fix_data_dir.sh: file data/train/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/spk2utt is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/segments is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/wav.scp is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/reco2dur is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 61890 utterances.
fix_data_dir.sh: old files are kept in data/train/.backup
utils/fix_data_dir.sh: file data/dev/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/spk2utt is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/segments is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/wav.scp is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/dev/reco2dur is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 7251 utterances.
fix_data_dir.sh: old files are kept in data/dev/.backup
2025-06-18T22:17:27 (data.sh:91:main) Successfully finished. [elapsed=9s]
2025-06-18T22:17:27 (diar.sh:200:main) Stage 2: Format wav.scp: data/ -> dump/raw
utils/copy_data_dir.sh: copied data from data/train to dump/raw/org/train
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/org/train
2025-06-18T22:17:28 (format_wav_scp.sh:46:main) scripts/audio/format_wav_scp.sh --nj 32 --cmd slurm.pl --audio-format flac --fs 8k data/train/wav.scp dump/raw/org/train
2025-06-18T22:17:29 (format_wav_scp.sh:118:main) [info]: without segments
2025-06-18T22:19:15 (format_wav_scp.sh:153:main) Successfully finished. [elapsed=107s]
/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1/steps/libs/common.py:127: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if p.returncode is not 0:
/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1/steps/libs/common.py:147: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if p.returncode is not 0:
/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1/steps/libs/common.py:203: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if popen_object.returncode is not 0:
2025-06-18 22:21:35,922 (convert_rttm:136) INFO: Successfully finished RTTM converting.
utils/copy_data_dir.sh: copied data from data/dev to dump/raw/org/dev
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/org/dev
2025-06-18T22:21:36 (format_wav_scp.sh:46:main) scripts/audio/format_wav_scp.sh --nj 32 --cmd slurm.pl --audio-format flac --fs 8k data/dev/wav.scp dump/raw/org/dev
2025-06-18T22:21:37 (format_wav_scp.sh:118:main) [info]: without segments
2025-06-18T22:22:39 (format_wav_scp.sh:153:main) Successfully finished. [elapsed=63s]
2025-06-18 22:22:49,341 (convert_rttm:136) INFO: Successfully finished RTTM converting.
utils/copy_data_dir.sh: copied data from data/test to dump/raw/test
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/test
2025-06-18T22:22:49 (format_wav_scp.sh:46:main) scripts/audio/format_wav_scp.sh --nj 32 --cmd slurm.pl --audio-format flac --fs 8k data/test/wav.scp dump/raw/test
2025-06-18T22:22:50 (format_wav_scp.sh:118:main) [info]: without segments
2025-06-18T22:23:36 (format_wav_scp.sh:153:main) Successfully finished. [elapsed=47s]
2025-06-18 22:23:44,798 (convert_rttm:136) INFO: Successfully finished RTTM converting.
2025-06-18T22:23:44 (diar.sh:241:main) Stage 3: Remove short data: dump/raw/org -> dump/raw
utils/copy_data_dir.sh: copied data from dump/raw/org/train to dump/raw/train
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/train
fix_data_dir.sh: kept all 61890 utterances.
fix_data_dir.sh: old files are kept in dump/raw/train/.backup
2025-06-18 22:23:51,881 (convert_rttm:136) INFO: Successfully finished RTTM converting.
utils/copy_data_dir.sh: copied data from dump/raw/org/dev to dump/raw/dev
utils/validate_data_dir.sh: Successfully validated data-directory dump/raw/dev
fix_data_dir.sh: kept all 7251 utterances.
fix_data_dir.sh: old files are kept in dump/raw/dev/.backup
2025-06-18 22:23:54,181 (convert_rttm:136) INFO: Successfully finished RTTM converting.
2025-06-18T22:23:54 (diar.sh:293:main) Stage 4: Diarization collect stats: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-06-18T22:23:54 (diar.sh:343:main) Generate 'exp/diar_stats_8k/run.sh'. You can resume the process from stage 4 using this script
2025-06-18T22:23:54 (diar.sh:347:main) Diarization collect-stats started... log: 'exp/diar_stats_8k/logdir/stats.*.log'
/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1/utils/slurm.pl: 32 / 32 failed, log is in exp/diar_stats_8k/logdir/stats.*.log
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=1
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892985
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892985
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62352
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.1.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.1.scp --output_dir exp/diar_stats_8k/logdir/stats.1 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] [Errno 2] No such file or directory: '/jet/home/ttao3/nltk
[nltk_data]     _data/taggers/averaged_perceptron_tagger.zip'
[nltk_data] Downloading package cmudict to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping corpora/cmudict.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.1.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.1.scp --output_dir exp/diar_stats_8k/logdir/stats.1 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.1/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.1', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.1.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.1.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,788 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=10
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892994
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892994
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62420
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.10.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.10.scp --output_dir exp/diar_stats_8k/logdir/stats.10 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.10.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.10.scp --output_dir exp/diar_stats_8k/logdir/stats.10 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,317 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.10/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.10', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.10.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.10.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,808 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=11
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892995
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892995
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62317
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.11.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.11.scp --output_dir exp/diar_stats_8k/logdir/stats.11 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.11.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.11.scp --output_dir exp/diar_stats_8k/logdir/stats.11 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,316 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,316 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.11/config.yaml
[r007] 2025-06-18 22:26:24,329 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.11', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.11.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.11.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,816 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=12
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892996
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892996
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62410
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.12.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.12.scp --output_dir exp/diar_stats_8k/logdir/stats.12 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.12.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.12.scp --output_dir exp/diar_stats_8k/logdir/stats.12 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,148 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,149 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,150 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,150 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,150 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,150 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,150 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,150 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,150 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,317 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.12/config.yaml
[r007] 2025-06-18 22:26:24,329 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.12', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.12.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.12.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,810 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=13
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892997
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892997
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62411
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.13.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.13.scp --output_dir exp/diar_stats_8k/logdir/stats.13 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.13.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.13.scp --output_dir exp/diar_stats_8k/logdir/stats.13 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,133 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,133 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,133 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,133 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,133 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,133 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.13/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.13', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.13.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.13.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,815 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=14
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892998
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892998
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62380
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.14.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.14.scp --output_dir exp/diar_stats_8k/logdir/stats.14 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.14.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.14.scp --output_dir exp/diar_stats_8k/logdir/stats.14 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.14/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.14', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.14.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.14.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,811 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:54 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=15
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892999
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892999
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=24113
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.15.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.15.scp --output_dir exp/diar_stats_8k/logdir/stats.15 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.15.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.15.scp --output_dir exp/diar_stats_8k/logdir/stats.15 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,790 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,791 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,792 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:27:01,794 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:27:01,795 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:27:01,796 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:27:01,796 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:27:01,797 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.15/config.yaml
[r007] 2025-06-18 22:27:01,805 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.15', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.15.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.15.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:27:02,202 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300014
# Accounting: end_time=1750300022
# Accounting: time=8 threads=1
# Finished at Wed Jun 18 22:27:02 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:40 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=17
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893001
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893001
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=65136
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.17.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.17.scp --output_dir exp/diar_stats_8k/logdir/stats.17 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.17.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.17.scp --output_dir exp/diar_stats_8k/logdir/stats.17 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:49,005 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:49,006 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:49,006 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:49,007 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:49,008 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.17/config.yaml
[r007] 2025-06-18 22:26:49,016 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.17', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.17.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.17.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:49,419 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300000
# Accounting: end_time=1750300010
# Accounting: time=10 threads=1
# Finished at Wed Jun 18 22:26:50 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=18
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893002
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893002
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62404
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.18.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.18.scp --output_dir exp/diar_stats_8k/logdir/stats.18 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] [Errno 2] No such file or directory: '/jet/home/ttao3/nltk
[nltk_data]     _data/taggers/averaged_perceptron_tagger.zip'
[nltk_data] Downloading package cmudict to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping corpora/cmudict.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.18.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.18.scp --output_dir exp/diar_stats_8k/logdir/stats.18 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,134 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.18/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.18', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.18.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.18.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,788 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:40 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=19
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893003
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893003
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=65239
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.19.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.19.scp --output_dir exp/diar_stats_8k/logdir/stats.19 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.19.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.19.scp --output_dir exp/diar_stats_8k/logdir/stats.19 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:49,001 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,002 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:49,003 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:49,005 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:49,007 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:49,007 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:49,007 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:49,008 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.19/config.yaml
[r007] 2025-06-18 22:26:49,016 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.19', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.19.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.19.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:49,421 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300000
# Accounting: end_time=1750300010
# Accounting: time=10 threads=1
# Finished at Wed Jun 18 22:26:50 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:40 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=20
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893004
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893004
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=65458
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.20.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.20.scp --output_dir exp/diar_stats_8k/logdir/stats.20 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.20.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.20.scp --output_dir exp/diar_stats_8k/logdir/stats.20 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:48,904 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:48,905 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:48,908 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:48,909 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:48,909 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:48,909 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:48,910 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.20/config.yaml
[r007] 2025-06-18 22:26:48,919 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.20', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.20.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.20.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:49,309 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300000
# Accounting: end_time=1750300009
# Accounting: time=9 threads=1
# Finished at Wed Jun 18 22:26:49 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:51 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=21
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893005
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893005
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62005
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.21.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.21.scp --output_dir exp/diar_stats_8k/logdir/stats.21 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] [Errno 2] No such file or directory: '/jet/home/ttao3/nltk
[nltk_data]     _data/taggers/averaged_perceptron_tagger.zip'
[nltk_data] Downloading package cmudict to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping corpora/cmudict.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.21.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.21.scp --output_dir exp/diar_stats_8k/logdir/stats.21 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.21/config.yaml
[r007] 2025-06-18 22:26:24,329 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.21', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.21.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.21.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,788 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299891
# Accounting: end_time=1750299986
# Accounting: time=95 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:51 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=22
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893006
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893006
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=13327
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.22.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.22.scp --output_dir exp/diar_stats_8k/logdir/stats.22 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.22.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.22.scp --output_dir exp/diar_stats_8k/logdir/stats.22 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:58,533 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:58,534 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:58,535 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:58,537 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:58,539 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:58,539 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:58,539 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:58,540 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.22/config.yaml
[r007] 2025-06-18 22:26:58,549 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.22', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.22.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.22.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:58,940 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300011
# Accounting: end_time=1750300019
# Accounting: time=8 threads=1
# Finished at Wed Jun 18 22:26:59 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=24
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893008
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893008
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62221
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.24.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.24.scp --output_dir exp/diar_stats_8k/logdir/stats.24 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Error with downloaded zip file
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 3, in <module>
    from espnet2.tasks.diar import DiarizationTask
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/diar.py", line 30, in <module>
    from espnet2.train.collate_fn import CommonCollateFn
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/collate_fn.py", line 11, in <module>
    from espnet2.train.preprocessor import detect_non_silence
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/preprocessor.py", line 18, in <module>
    from espnet2.text.build_tokenizer import build_tokenizer
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/text/build_tokenizer.py", line 9, in <module>
    from espnet2.text.phoneme_tokenizer import PhonemeTokenizer
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/text/phoneme_tokenizer.py", line 7, in <module>
    import g2p_en
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/g2p_en/__init__.py", line 1, in <module>
    from .g2p import G2p
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/g2p_en/g2p.py", line 26, in <module>
    nltk.data.find('corpora/cmudict.zip')
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/nltk/data.py", line 538, in find
    return ZipFilePathPointer(p, zipentry)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/nltk/data.py", line 391, in __init__
    zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/nltk/data.py", line 1020, in __init__
    zipfile.ZipFile.__init__(self, filename)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/zipfile.py", line 1268, in __init__
    self._RealGetContents()
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/zipfile.py", line 1335, in _RealGetContents
    raise BadZipFile("File is not a zip file")
zipfile.BadZipFile: File is not a zip file
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:28 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=25
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893009
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893009
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=64237
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.25.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.25.scp --output_dir exp/diar_stats_8k/logdir/stats.25 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.25.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.25.scp --output_dir exp/diar_stats_8k/logdir/stats.25 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:36,099 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:36,100 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:36,103 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:36,104 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:36,104 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:36,104 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:36,105 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.25/config.yaml
[r007] 2025-06-18 22:26:36,114 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.25', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.25.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.25.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:36,503 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299988
# Accounting: end_time=1750299997
# Accounting: time=9 threads=1
# Finished at Wed Jun 18 22:26:37 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:38 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=26
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893010
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893010
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=65033
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.26.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.26.scp --output_dir exp/diar_stats_8k/logdir/stats.26 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.26.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.26.scp --output_dir exp/diar_stats_8k/logdir/stats.26 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:46,328 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:46,329 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:46,332 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:46,333 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:46,333 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:46,333 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:46,334 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.26/config.yaml
[r007] 2025-06-18 22:26:46,343 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.26', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.26.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.26.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:46,741 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299998
# Accounting: end_time=1750300007
# Accounting: time=9 threads=1
# Finished at Wed Jun 18 22:26:47 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=27
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893011
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893011
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62393
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.27.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.27.scp --output_dir exp/diar_stats_8k/logdir/stats.27 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Error with downloaded zip file
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 3, in <module>
    from espnet2.tasks.diar import DiarizationTask
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/diar.py", line 30, in <module>
    from espnet2.train.collate_fn import CommonCollateFn
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/collate_fn.py", line 11, in <module>
    from espnet2.train.preprocessor import detect_non_silence
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/preprocessor.py", line 18, in <module>
    from espnet2.text.build_tokenizer import build_tokenizer
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/text/build_tokenizer.py", line 9, in <module>
    from espnet2.text.phoneme_tokenizer import PhonemeTokenizer
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/text/phoneme_tokenizer.py", line 7, in <module>
    import g2p_en
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/g2p_en/__init__.py", line 1, in <module>
    from .g2p import G2p
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/g2p_en/g2p.py", line 26, in <module>
    nltk.data.find('corpora/cmudict.zip')
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/nltk/data.py", line 538, in find
    return ZipFilePathPointer(p, zipentry)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/nltk/data.py", line 391, in __init__
    zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/nltk/data.py", line 1020, in __init__
    zipfile.ZipFile.__init__(self, filename)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/zipfile.py", line 1268, in __init__
    self._RealGetContents()
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/zipfile.py", line 1335, in _RealGetContents
    raise BadZipFile("File is not a zip file")
zipfile.BadZipFile: File is not a zip file
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=28
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893012
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893012
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62409
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.28.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.28.scp --output_dir exp/diar_stats_8k/logdir/stats.28 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.28.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.28.scp --output_dir exp/diar_stats_8k/logdir/stats.28 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.28/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.28', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.28.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.28.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,806 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=29
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893013
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893013
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62398
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.29.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.29.scp --output_dir exp/diar_stats_8k/logdir/stats.29 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.29.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.29.scp --output_dir exp/diar_stats_8k/logdir/stats.29 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.29/config.yaml
[r007] 2025-06-18 22:26:24,329 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.29', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.29.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.29.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,809 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:53 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=3
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892987
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892987
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=24089
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.3.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.3.scp --output_dir exp/diar_stats_8k/logdir/stats.3 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.3.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.3.scp --output_dir exp/diar_stats_8k/logdir/stats.3 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,670 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,671 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,672 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:27:01,674 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:27:01,675 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:27:01,675 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:27:01,675 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:27:01,677 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.3/config.yaml
[r007] 2025-06-18 22:27:01,685 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.3', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.3.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.3.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:27:02,077 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300013
# Accounting: end_time=1750300022
# Accounting: time=9 threads=1
# Finished at Wed Jun 18 22:27:02 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=30
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893014
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893014
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62360
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.30.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.30.scp --output_dir exp/diar_stats_8k/logdir/stats.30 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.30.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.30.scp --output_dir exp/diar_stats_8k/logdir/stats.30 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,174 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,174 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,174 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,175 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,176 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.30/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.30', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.30.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.30.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,809 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=31
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32893015
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32893015
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62359
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.31.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.31.scp --output_dir exp/diar_stats_8k/logdir/stats.31 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.31.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.31.scp --output_dir exp/diar_stats_8k/logdir/stats.31 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,140 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,141 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,142 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,317 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.31/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.31', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.31.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.31.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,814 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=32
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892100
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892100
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62414
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.32.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.32.scp --output_dir exp/diar_stats_8k/logdir/stats.32 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Error with downloaded zip file
[nltk_data] Downloading package cmudict to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping corpora/cmudict.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.32.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.32.scp --output_dir exp/diar_stats_8k/logdir/stats.32 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,168 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,168 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,168 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,169 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,170 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,317 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.32/config.yaml
[r007] 2025-06-18 22:26:24,329 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.32', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.32.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.32.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,786 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=4
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892988
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892988
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62358
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.4.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.4.scp --output_dir exp/diar_stats_8k/logdir/stats.4 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] [Errno 2] No such file or directory: '/jet/home/ttao3/nltk
[nltk_data]     _data/taggers/averaged_perceptron_tagger.zip'
[nltk_data] Downloading package cmudict to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping corpora/cmudict.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.4.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.4.scp --output_dir exp/diar_stats_8k/logdir/stats.4 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.4/config.yaml
[r007] 2025-06-18 22:26:24,329 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.4', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.4.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.4.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,798 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:54 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=5
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892989
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892989
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=24093
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.5.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.5.scp --output_dir exp/diar_stats_8k/logdir/stats.5 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.5.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.5.scp --output_dir exp/diar_stats_8k/logdir/stats.5 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:27:01,776 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:27:01,776 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:27:01,776 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,776 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,776 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,777 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,778 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:27:01,780 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:27:01,782 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:27:01,782 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:27:01,782 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:27:01,797 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.5/config.yaml
[r007] 2025-06-18 22:27:01,805 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.5', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.5.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.5.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:27:02,195 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300014
# Accounting: end_time=1750300022
# Accounting: time=8 threads=1
# Finished at Wed Jun 18 22:27:02 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:46 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=6
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892990
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892990
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=91693
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.6.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.6.scp --output_dir exp/diar_stats_8k/logdir/stats.6 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.6.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.6.scp --output_dir exp/diar_stats_8k/logdir/stats.6 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:53,586 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:53,586 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:53,586 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:53,586 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:53,587 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:53,588 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:53,591 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:53,592 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:53,592 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:53,592 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:53,593 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.6/config.yaml
[r007] 2025-06-18 22:26:53,602 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.6', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.6.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.6.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:54,009 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300006
# Accounting: end_time=1750300014
# Accounting: time=8 threads=1
# Finished at Wed Jun 18 22:26:54 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=7
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892991
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892991
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62379
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.7.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.7.scp --output_dir exp/diar_stats_8k/logdir/stats.7 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Error with downloaded zip file
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.7.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.7.scp --output_dir exp/diar_stats_8k/logdir/stats.7 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,135 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,136 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,316 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.7/config.yaml
[r007] 2025-06-18 22:26:24,328 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.7', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.7.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.7.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,798 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:24:52 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=8
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892992
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892992
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=62194
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.8.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.8.scp --output_dir exp/diar_stats_8k/logdir/stats.8 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /jet/home/ttao3/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.8.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.8.scp --output_dir exp/diar_stats_8k/logdir/stats.8 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,137 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,138 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:26:24,139 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:26:24,298 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:26:24,317 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:26:24,317 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:26:24,317 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:26:24,320 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.8/config.yaml
[r007] 2025-06-18 22:26:24,329 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.8', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.8.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.8.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:26:24,811 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750299892
# Accounting: end_time=1750299986
# Accounting: time=94 threads=1
# Finished at Wed Jun 18 22:26:26 EDT 2025 with status 1
# Running on r007.ib.bridges2.psc.edu
# Started at Wed Jun 18 22:26:53 EDT 2025
# SLURMD_NODENAME=r007
# SLURM_ARRAY_JOB_ID=32892100
# SLURM_ARRAY_TASK_COUNT=32
# SLURM_ARRAY_TASK_ID=9
# SLURM_ARRAY_TASK_MAX=32
# SLURM_ARRAY_TASK_MIN=1
# SLURM_ARRAY_TASK_STEP=1
# SLURM_CLUSTER_NAME=bridges2
# SLURM_CONF=/var/spool/slurm/d/conf-cache/slurm.conf
# SLURM_CPUS_ON_NODE=1
# SLURM_CPU_BIND=quiet,mask_cpu:0x00000F8000
# SLURM_CPU_BIND_LIST=0x00000F8000
# SLURM_CPU_BIND_TYPE=mask_cpu:
# SLURM_CPU_BIND_VERBOSE=quiet
# SLURM_EXPORT_ENV=ALL
# SLURM_GET_USER_ENV=1
# SLURM_GTIDS=0
# SLURM_JOBID=32892993
# SLURM_JOB_ACCOUNT=cis210027p
# SLURM_JOB_CPUS_PER_NODE=1
# SLURM_JOB_GID=24886
# SLURM_JOB_ID=32892993
# SLURM_JOB_NAME=stats.sh
# SLURM_JOB_NODELIST=r007
# SLURM_JOB_NUM_NODES=1
# SLURM_JOB_PARTITION=RM-shared
# SLURM_JOB_QOS=rm
# SLURM_JOB_UID=97947
# SLURM_JOB_USER=ttao3
# SLURM_LAUNCH_NODE_IPADDR=10.8.11.32
# SLURM_LOCALID=0
# SLURM_MEM_PER_CPU=1900
# SLURM_NNODES=1
# SLURM_NODEID=0
# SLURM_NODELIST=r007
# SLURM_NODE_ALIASES='(null)'
# SLURM_NPROCS=1
# SLURM_NTASKS=1
# SLURM_OPEN_MODE=a
# SLURM_PRIO_PROCESS=0
# SLURM_PROCID=0
# SLURM_PTY_PORT=36273
# SLURM_PTY_WIN_COL=150
# SLURM_PTY_WIN_ROW=15
# SLURM_SCRIPT_CONTEXT=prolog_task
# SLURM_SRUN_COMM_HOST=10.8.11.32
# SLURM_SRUN_COMM_PORT=36945
# SLURM_STEPID=0
# SLURM_STEP_GPUS=3
# SLURM_STEP_ID=0
# SLURM_STEP_LAUNCHER_PORT=36945
# SLURM_STEP_NODELIST=v031
# SLURM_STEP_NUM_NODES=1
# SLURM_STEP_NUM_TASKS=1
# SLURM_STEP_TASKS_PER_NODE=1
# SLURM_SUBMIT_DIR=/ocean/projects/cis210027p/ttao3/espnet/egs2/librimix/diar1
# SLURM_SUBMIT_HOST=v031.ib.bridges2.psc.edu
# SLURM_TASKS_PER_NODE=1
# SLURM_TASK_PID=24092
# SLURM_TOPOLOGY_ADDR=r007
# SLURM_TOPOLOGY_ADDR_PATTERN=node
# SLURM_WORKING_CLUSTER=bridges2:br003:6810:9728:109
# python3 -m espnet2.bin.diar_train --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.9.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.9.scp --output_dir exp/diar_stats_8k/logdir/stats.9 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2 
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/bin/python3 /ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py --collect_stats true --use_preprocessor true --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/train/espnet_rttm,spk_labels,rttm --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/dev/espnet_rttm,spk_labels,rttm --train_shape_file exp/diar_stats_8k/logdir/train.9.scp --valid_shape_file exp/diar_stats_8k/logdir/valid.9.scp --output_dir exp/diar_stats_8k/logdir/stats.9 --config conf/train_diar_eda.yaml --frontend_conf fs=8k --frontend_conf hop_length=128 --num_spk 2
/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.embed.0.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.embed.1.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.0.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.1.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,718 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.2.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.2.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.norm1.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.encoders.3.norm2.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize encoder.after_norm.bias to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize attractor.attractor_encoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_ih_l0 to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize attractor.attractor_decoder.bias_hh_l0 to zeros
[r007] 2025-06-18 22:27:01,719 (initialize:90) INFO: Initialize attractor.linear_projection.bias to zeros
[r007] 2025-06-18 22:27:01,722 (abs_task:1398) INFO: pytorch.version=1.13.1, cuda.available=False, cudnn.version=8500, cudnn.benchmark=False, cudnn.deterministic=True
[r007] 2025-06-18 22:27:01,723 (abs_task:1399) INFO: Model structure:
ESPnetDiarizationModel(
  (encoder): TransformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=80, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): ReLU()
      (4): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=8000, n_fft=512, n_mels=80, fmin=0, fmax=4000.0, htk=False)
  )
  (specaug): SpecAug(
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (label_aggregator): LabelAggregate(win_length=512, hop_length=128, center=True, )
  (attractor): RnnAttractor(
    (attractor_encoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (attractor_decoder): LSTM(256, 256, batch_first=True, dropout=0.1)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (linear_projection): Linear(in_features=256, out_features=1, bias=True)
  )
  (decoder): None
)

Model summary:
    Class Name: ESPnetDiarizationModel
    Total Number of model parameters: 3.18 M
    Number of trainable parameters: 3.18 M (100.0%)
    Size: 12.73 MB
    Type: torch.float32
[r007] 2025-06-18 22:27:01,723 (abs_task:1402) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.002
    lr: 6.666666666666667e-08
    maximize: False
    weight_decay: 0
)
[r007] 2025-06-18 22:27:01,723 (abs_task:1403) INFO: Scheduler: WarmupLR(warmup_steps=30000)
[r007] 2025-06-18 22:27:01,724 (abs_task:1412) INFO: Saving the configuration in exp/diar_stats_8k/logdir/stats.9/config.yaml
[r007] 2025-06-18 22:27:01,732 (abs_task:1423) INFO: Namespace(config='conf/train_diar_eda.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/diar_stats_8k/logdir/stats.9', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, use_deepspeed=False, deepspeed_config=None, gradient_as_bucket_view=True, ddp_comm_hook=None, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, use_tf32=False, collect_stats=True, write_collected_feats=False, max_epoch=250, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'acc', 'max']], keep_nbest_models=10, nbest_averaging_interval=0, grad_clip=5, grad_clip_type=2.0, grad_noise=False, accum_grad=6, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_adapter=False, adapter='lora', save_strategy='all', adapter_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=None, batch_size=20, valid_batch_size=None, batch_bins=15000000, valid_batch_bins=None, category_sample_size=10, train_shape_file=['exp/diar_stats_8k/logdir/train.9.scp'], valid_shape_file=['exp/diar_stats_8k/logdir/valid.9.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, chunk_max_abs_length=None, chunk_discard_short_samples=True, train_data_path_and_name_and_type=[('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/raw/train/espnet_rttm', 'spk_labels', 'rttm')], valid_data_path_and_name_and_type=[('dump/raw/dev/wav.scp', 'speech', 'sound'), ('dump/raw/dev/espnet_rttm', 'spk_labels', 'rttm')], multi_task_dataset=False, allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.002}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, num_spk=2, init='xavier_uniform', input_size=None, model_conf={'diar_weight': 1.0, 'attractor_weight': 1.0}, use_preprocessor=True, frontend='default', frontend_conf={'fs': '8k', 'hop_length': 128}, specaug='specaug', specaug_conf={'apply_time_warp': False, 'apply_freq_mask': True, 'freq_mask_width_range': [0, 30], 'num_freq_mask': 2, 'apply_time_mask': True, 'time_mask_width_range': [0, 40], 'num_time_mask': 2}, normalize='utterance_mvn', normalize_conf={}, encoder='transformer', encoder_conf={'input_layer': 'linear', 'num_blocks': 4, 'linear_units': 512, 'dropout_rate': 0.1, 'output_size': 256, 'attention_heads': 4, 'attention_dropout_rate': 0.1}, decoder='linear', decoder_conf={}, label_aggregator='label_aggregator', label_aggregator_conf={}, attractor='rnn', attractor_conf={'unit': 256, 'layer': 1, 'dropout': 0.1, 'attractor_grad': True}, required=['output_dir'], version='202503', distributed=False)
[r007] 2025-06-18 22:27:02,125 (dataset:541) ERROR: An error happened with RttmReader(dump/raw/train/espnet_rttm)
Traceback (most recent call last):
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 23, in <module>
    main()
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/bin/diar_train.py", line 19, in main
    DiarizationTask.main(cmd=cmd)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1225, in main
    cls.main_worker(args)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 1443, in main_worker
    train_iter=cls.build_streaming_iterator(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/tasks/abs_task.py", line 2273, in build_streaming_iterator
    dataset = dataset_class(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/iterable_dataset.py", line 122, in __init__
    self.non_iterable_dataset = ESPnetDataset(
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 483, in __init__
    loader = self._build_loader(path, _type, keys_to_load)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/train/dataset.py", line 535, in _build_loader
    return func(path, **kwargs)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 76, in __init__
    self.data = load_rttm_text(path=fname)
  File "/ocean/projects/cis210027p/ttao3/espnet/espnet2/fileio/rttm.py", line 42, in load_rttm_text
    return data
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_functions.py", line 166, in check_return_type
    check_type_internal(retval, annotation, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 234, in check_mapping
    check_type_internal(v, value_type, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 960, in check_type_internal
    checker(value, origin_type, args, memo)
  File "/ocean/projects/cis210027p/ttao3/miniconda3/envs/espnet/lib/python3.9/site-packages/typeguard/_checkers.py", line 295, in check_list
    raise TypeCheckError("is not a list")
typeguard.TypeCheckError: value of key '103-1240-0003_1235-135887-0017' of the return value (dict) is not a list
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn_2_cuda'
# Accounting: begin_time=1750300013
# Accounting: end_time=1750300022
# Accounting: time=9 threads=1
# Finished at Wed Jun 18 22:27:02 EDT 2025 with status 1
